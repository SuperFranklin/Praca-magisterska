\section{Skalowalność i wysoka dostępność}
Rozdział rozpoczyna się od wyjaśnienia terminów i teorii, które będą przydatne podczas dalszego rozważania zagadnień wydajności, skalowalności i wysokiej dostępności MySQL.


\subsection{Terminologia}
\subsubsection{Skalowalność a wydajność}
Celem tego podrozdziału jest wyjaśnienie różnicy pomiędzy skalowalnością, a wydajnością. Termin\textit{wydajność} w informatyce dotyczy ilości danych przetwarzanych w czasie. W przypadku baz danych termin ten może dotyczyć: ilości jednoczesnych połączeń do bazy danych, liczbie zapytań wykonywanych na sekundę lub rozmiar odczytywanych danych. Skalowalność oznacza możliwość aplikacji do zwiększenia wydajności. Skalowalny system to taki, w którym w najgorszym przypadku wzrost kosztów wynikający ze zwiększenia zasobów jest liniowy do wzrostu wydajności, jakie te zasoby zapewniają. 

Możliwy jest system, który jest bardzo wydajny, ale bardzo słabo skalowany. Możliwy jest także system niewydajny, który jest bardzo dobrze skalowany, dlatego istotnym jest, aby nie mylić tych pojęć.


\subsubsection{Teoria CAP}
Autorem teorii CAP jest Eric Brewer, który przypisał bazą danych trzy własności:
\begin{itemize}
	\item Spójność (eng. \textit{Consistency}), oznaczająca, że odpytując dowolny działający węzeł, zawsze otrzymamy takie same dane.
	\item Dostępność (eng. \textit{Availability}), określająca możliwość zapisywania i odczytywania danych nawet w przypadku awarii dowolnego węzła.
	\item Odporność na podział (eng. \textit{Partition Tolerance}), pozwalająca na rozproszenie niewrażliwe na awarię.
\end{itemize}
Istotą teorii CAP jest stwierdzenie, że baza danych może spełniać co najwyżej dwie spośród trzech wymienionych wyżej własności. Konkluzją z powyższego stwierdzenia jest nieistnienie idealnej bazy danych i każda z nich jest pewnym kompromisem, kładącym nacisk na pewne własności, kosztem innych.

\subsubsection{Skalowanie wertykalne i horyzontalne}
Skalowaniem wertykalnym nazywamy zwiększenie wydajności w ramach pojedyńczego serwera. Zwiększenie wydajności polega na dodawaniu kolejnych zasobów takich jak pamięć czy procesor. Takie rozwiązanie jest skuteczne tylko do pewnego momentu. Wraz z wzrostem wydajności, rozwiązanie to będzie się wiązało z dużym wzrostem kosztów, nawet przy małym wzroście możliwości serwera. Ostatecznie osiągniemy limit możliwości jednej maszyny (serwera) i zwiększenie wydajności nie będzie dalej możliwe. 

Dodatkowo sam serwer MySQL ma problemy z wykorzystywaniem większej ilości procesorów i dysków twardych. Kolejnym utrudnieniem jest fakt, że MySQL nie potrafi obsłużyć pojedyńczego zapytania na kilku procesorach, co jest sporym utrudnieniem przy zapytaniach, które mocno obciążają procesor maszyny. Z powodu wyżej wymienionych ograniczeń skalowania wertykalnego, nie nadaje się ono, jeżeli aplikacja ma zapewnić wysoką skalowalność.

W modelu skalowalności horyzontalnej zwiększenie wydajności odbywa się przez dodanie kolejnych serwerów, przy zachowaniu wydajności pojedynczych serwerów. W kolejnych podrozdziałach zostaną przedstawione różne sposoby realizacji modelu skalowania horyzontalnego. Takie rozwiązanie jest zdecydowanie tańsze i teoretycznie nie posiada limitu wydajności. Niestety wraz ze wzrostem liczby serwerów, rosną też problemy z zachowaniem spójności pomiędzy poszczególnymi serwerami (węzłami).


\subsection{Architektura master-slave}
Najpopularniejszym sposobem skalowania horyzontalnego jest realizacja architektury \textit{master-slave}. Rozwiązanie polega na podziale serwerów bazodanowych na jeden serwer nadrzędny (\textit{master}) oraz wiele serwerów podrzędnych (\textit{slave}). Operacje modyfikujące dane wykonywane są na serwerze nadrzędnym, a operacje odczytu mogą być wykonywane na każdym z serwerów. Najczęściej operacje odczytu wykonywane są tylko na serwerach nadrzędnych celem odciążenia serwera \textit{master}. Replikacja danych pomiędzy serwerem nadrzędnym, a podrzędnymi opiera się na następującej zasadzie. Serwer nadrzędny obsługuje wszystkie operacje, które modyfikują dane (zapis, odczyt, aktualizacja), równocześnie rejestrując każdą operację, która wykonał. Następnie każdy z serwerów podrzędnych odczytuje rejestr operacji i aktualizuje swój stan. Efektem tej pracy jest wiele instancji bazy danych zawierających identyczne dane. Możliwe jest takie skonfigurowanie mechanizmu replikacji, żeby replikowane były dane z tylko niektórych schematów, lub nawet pojedynczych tabel.


Taki podział ma wiele zalet. Przede wszystkim wyraźnie zwiększone zostaje wydajość operacji odczytu, ponieważ mogą być realizowane równolegle przez wiele instacji serwera MySQL. Dodatkowo redundancja danych na wielu instancjach sprawia, że system staje się bardziej odporny na utratę danych spowodowaną awarią sprzętową. W przypadku awarii na którymkolwiek z serwerów, dane na innych instancjach umożliwią przywrócenie stanu sprzed awarii. Dodatkowo zmniejsza się zapotrzebowanie na tworzenie kopi zapasowych danych, co jest szcególnie przydatne przy dużych rozmiarach danych, ponieważ operacje tworzenia kopi zapasowych mogą być znaczącym obciążeniem dla serwera. Co więcej, operację \textit{backupu} danych można wykonać na serwerze \textit{slave}, co dodatkowo odciąża serwer główny. Replikacja danych w trybie \textit{master-slave} może odbywać się w jednym z następujących trybów:
\begin{itemize}
 	\item \textbf{SBR} (statement-based replication) - Najprostsza z metod. Zapisywane są tylko zapytania modyfikujące dane. SBR jest pierwszym typem replikacji stosowanym w MySQL. Tryb ten może jednak prowadzić do braku spójności danych na różnych serwerach \textit{slave}. Wyobraźmy sobie zapytanie, które zawiera funckję RAND() lub funkcje odwołujące się do aktualnego czasu. Takie zapytania wykonane na różnych serwerach mogą zmodyfikować dane w taki sposób, że będą one niespójne, ponieważ funkcja RAND() może zwrócić różne wyniki na różnych serwerach.
	\item \textbf{RBR} (row-based replication) - logowane są wyniki zapytań modyfikujących dane, czyli informacja o tym, który rekord i w jaki sposób został zmodyfikowany. Jest to domyślny typ replikacji w MYSQL 8. Jego wadą w stosunku do metody SBR jest mniejsza szybkość oraz większa ilość danych przesyłanych pomiędzy serweram nadrzędnym i podrzędnymi.
	\item \textbf{MFL} - połączenie dwóch powyższych metod. W tym trybie domyślnie używana jest metoda SBR, ale w niektórych sytuacjach wykorzystywana jest metoda RBR.
\end{itemize}

Rozwiązanie polegające na replikacji danych do serwerów podległych idealnie sprawdzi się w aplikacjach, które przechowują ograniczoną wielkość danych, oraz zdecydowana większość operacji, to operacje odczytu. Taka realizacja skalowania poziomego może nie sprawdzić się w systemach przechowujących ogromne ilości danych, ponieważ wymaga przechowywania wszystkich danych w każdym z węzłów, co może skutkować bardzo wysokimi kosztami finansowymi utrzymania serwerów, a nawet być niemożliwe, jeżeli rozmiar danych przekracza możliwości pojedynczego węzła. 

Kolejmym przypadkiem, którego nie rozwiązuje mechanizm replikacji jest system, w którym większość operacji to operacje modyfikujące dane. W takim scenariuszu serwer nadrzędny stanie się wąskim gardłem, a dodatkowo dużą część obciążenia serwerów będzie stanowiła replikacja pomiędzy serwerem nadrzędnym i serwerami podrzędnymi, co dodatkowo zmniejszy wydajność operacji zapisów na serwerze nadrzędnym, który część zasobów będzie przeznaczał na przygotowanie plików replikacji.

Reasumując skalowanie horyzontalne za pomocą mechanizmu replikacji \textit{master-slave} idealnie sprawdzi się , gdy zdecydowaną większością operacji są operacje odczytu, a rozmiar danych jest ograniczony.

Skalowanie z zastosowaniem replikacji \textit{master-slave} jest często stosowane razem z \textit{load balancerem} (przykładowo ProxySQL), który odpowiada za balansowanie obciążenia operacji odczytu pomiędzy serwerami podrzędnymi oraz zapewnia przeźroczystość dostępu do danych z punktu widzenia aplikacji, która ma wrażenie komunikowania się z tylko jednym serwerem. Przykład takiej architektura przedstawiono na poniższym diagramie.

\begin{figure}[!h]
	\caption{Przykładowa architektura master-slave z load balancerem}
	\centering
	\includegraphics[scale = 0.35]{architektura-z-load-balancerem.png}
	\label{fig:label}
\end{figure}

\subsection{Architektura multi-master}
\textit{Multi-master}, to architektura w której kilka serwerów pełni rolę serwera nadrzędnego (\textit{master}). W takim modelu każdy z serwerów przechowuje pełny zestaw danych oraz może modyfikować je w dowolnym momencie, które następnie są propagowane do pozostałych serwerów. Węzły odpowiadają także za rozwiązywanie potencjalnych konfliktów, które powstały w wyniku równoległych zmian na kilku instancjach. W MySQL realizacją takiej architektury jest replikacją grupową. Replikacja grupowa zapewnia spójność danych pomiędzy serwerami oraz dostepność nawet w przypadku awarii jednego z węzłów. Co istotne, jeżeli jeden z węzłów będzie nieaktywny, klienci z aktywnym połączeniem do tego serwera muszą zostać przełączeni na inny aktywny serwer. Replikacja grupowa nie posiada takiego mechanizmu, dlatego przełączanie ruchu pomiędzy serwerami master powinno być obsłużone na poziomie aplikacji, a najlepiej poprzez umieszczenie \textit{load balancera} pomiędzy aplikacją i serwerami należącymi do grupy.

Wykonanie zmiany w danych na jednym z serwerów, informacja o zmianie przesyłana jest do pozostałych węzłów w postaci plików \textit{binlong}.

Replikacja grupowa jest modelem replikacji optymistycznej. W tym podejściu zakłada się, że wiekszość operacji modyfikacji danych nie będzie powodowała problemów w spójności danych, dlatego nie stosuje się blokad w dostępie do danych, które nie zostały jeszcze zatwierdzone przez wszystkie serwery. Takie założenie może prowadzić do sytuacji, kiedy klient odpytujący dwa różne serwery może otrzymać różne wyniki (brak silnej spójności danych), ale współbieżne modyfikacje są kosztem, który został poniesiony w celu osiągnięcia większej wydajności i skalowalności grupy. W przypadku wystąpienia konfliktów pomiędzy transakcjami, potencjalne konfikty rozwiązywane są po zmodyfikowaniu danych. Z tego powodu ważne jest zachowanie dyscypliny po stronie aplikacji, żeby transakcje operujące na tych samych wierszach w miarę możliwości wykonywane były w ramach pojedynczej transakcji. Istotnym problemem może być wykorzystanie autoinkrementowanych kluczy głównych. Jeżeli dwa lub więcej serwerów będzie przydzielało klucze główne w dokładnie taki sam sposób, doprowadzi to do naruszenia kluczy podstawowych, dlatego bardzo ważne jest uważne ustawienie strategi generowania autoinkrementowanych kluczy podstawowych.

Architektura \textit{multi-master} będzie szczególnie przydatna, kiedy potrzebujemy zapewnić skalowalność operacji zapisu. Dodatkowo realizacja w postaci grupy serwerów zapewnia wygodne skalowanie poprzez elastyczne dodawanie lub odejmowanie węzłów, które zostaną automatycznie dołączone lub odłączone w sposób przeźroczysty dla aplikacji. Jedną z głównych wad realizacji architektury \textit{master-slave} w MySQL jest brak silnej spójności danych, który jednak w zdecydowanej większości sytuacji nie powinien stanowić istotnego problemu. Do wad z pewnością należy zaliczyć również większą złożoność systemu oraz bardziej skomplikowany proces konfiguracji w porównaniu do architektury \textit{master-slave}.


\subsection{Partycjonowanie funkcjonalne}

Funkcje aplikacji można podzielić na pewne podgrupy, które nie łączą się z pozostałymi, na poziomie zapytań SQL. Przykładowo serwis społecznościowy \textit{Facebook} umożliwia odczytywania postów innych użytkoników oraz dokonywanie zakupów w sekcji \textit{Marketplace}. Jeżeli użytkownik przegląda aktualne oferty w dziale \textit{Marketplace}, to zapytania kierowane do bazy danych, będą odpytywać jedynie kilka tabel związanych z zakupami. Jeżeli w tym samym czasie inny użytkownik przegląda posty użytkowników, zapytania nie będą dotyczyć table związanych z zakupami. Oczywiście przykład, który przedstawiłem powyżej, może być rozwiązany za pomocą podziału na osobne serwisy jeszcze na poziomie aplikacji, ale zakładając, że nasza aplikacja nie została podzielona na osobne serwisy i łączy się z jedną bazą danych. W takiej sytuacji obciążenie serwera MySQL jest sumą obciążeń związanych z postami i zakupami. W takim przypadku skutecznym rozwiązaniem jest podział danych pojedynczej aplikacji na zestaw tabel, które nigdy nie są ze sobą łączone. Oczywiście takiego partycjonowania danych nie można przeprowadzać w nieskończoność, ponieważ nie istnieje skończony zbiór tabel, które możemy w taki sposób podzielić. Dodatkowo w ramach każdej z grup jesteśmy ograniczeni możliwościami skalowania pionowego bazy danych. Wadą jest też zwiększona złożoność samej aplikacji, która musi obsłużyć kilka źródeł danych. 

\begin{center}
	\includegraphics[scale = 0.7]{partycjonowanie-funkcjonalne.png} 
\end{center}


\subsection{Data-sharding}
 
\textit{Data-sharding} wykorzystuje fakt, że rekordy w ramach pojedynczej tabeli są niezależne od siebie. Jako przykład przeanalizowano tabelę \textit{Comments} z testowej bazy \textit{StackOverflow}.
Wiersze w tabeli \textit{Comments} są logicznie powiązane z wierszami w tabeli \textit{Posts}, ale pomiędzy poszczególnymi wierszami w tabeli \textit{Comments} należącymi do innych postów, nie ma relacji. W naszej testowej bazie danych przechowujemy 25 milionów komentarzy. Z obsługą takiej ilości danych, baza danych nie powinna mieć problemów. Niestety z czasem tabela zawierająca komentarze może rozrosnąć się do rozmiarów, których obsłużenie w pojedynczej bazie danych może być problematyczne. Rozwiązaniem tego problemu może być podział pojedynczej tabeli na kilka serwerów. Przykładowo posty wraz z komentarzami o parzystym \textit{PostId} przechowywać na serwerze A, a nieparzyste na serwerze B. Dzięki temu, dwukrotnie zmniejszyliśmy liczbę zapytań do pojedynczej bazy danych, ilość wymaganego miejsca do przechowania postów i komentarzy, rozmiary buforów i indeksów. Wadą takiego rozwiązania jest konieczność obsługi sterowania zapytań do konkretnej bazy na poziomie aplikacji oraz wzrost złożoności logiki aplikacji i systemu bazy danych. Przykładowo chcąc pobrać najnowsze dziesięć postów, musimy wykonać dwa osobne zapytania, pobrać część redundatnych danych i na poziomie aplikacji dokonać wyboru dziesięciu najnowszych postów.


Głownymi przesłankami skłaniającymi do zastosowania tego sposobu skalowalności jest konieczność przechowywania danych w rozmiarach przekraczających możliwości jednego serwera lub skalowanie operacji zapisów, którego nie da się dokonać w ramach replikacji z jednym serwerem nadrzędnym. Sharding danych bardzo dobrze sprawdza się przy jednoczesnym zastosowaniu partycjonowania funkcjonalnego. Przykład zastosowania \textit{data-sharding} wraz z partycjonowaniem funkcjonalnym przedstawiono na poniższym schemacie.

\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.5]{Partycjonowanie-funkcjonalne-sharding.png}
	\caption{Przykładowa architektura partycjonowania funkcjonalnego}
	\label{fig:label}
\end{figure}


\subsection{InnoDB Cluster}

\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.35]{InnoDb-cluster-architektura.png}
	\caption{Architektura InnoDB Cluster}
	\label{fig:label}
\end{figure}
InnoDb Cluster jest dostępną natywnie kombinacją kilku technologi umożliwiających stworzenie wysoko dostępnej bazy danych w klastrze. Składa się z trzech podstawowych elementów:
\begin{itemize}
	\item \textbf{Group replication} czyli rozwiązania, które zostało opisane w poprzednich rozdziałach (architektura multi-master oraz master-slave).
	\item \textbf{MySQL Shell}, który jest klientem umożliwiającym zarządzanie serwerami MySQL za pomocą jezyków \textit{Java Script}, \textit{Python} lub SQL. Za jego pomocą możemy konfigurować klaster, między innymi poprzez dodawanie lub usuwanie węzłów, tworzenie nowych instancji serwerów, a nawet modyfikowanie danych na poszczególnych serwerach.
	\item \textbf{MySQL Router} będący punktem styku pomiędzy apliakacjami, a instancjami serwerów w klastrze. Router odpowiada za kierowanie ruchem do poszczególnych węzłów jednocześnie uniezależnia klientów od zmian wewnątrz klastra. 
\end{itemize}

Rozwiązanie InnoDB Cluster posiada wszystkie ograniczenia replikacji grupowej przedstawionej w poprzednich sekcjach. Zaletą tego rozwiązania jest wygodniejsza konfiguracja, dzięki możliwości wykorzystania \textit{MySQL shell} w którym możemy administrować naszym klastrem nawet wykorzystując takie języki jak \textit{Java script} czy \textit{Python}. Ważne jest to, że MySQL InnoDB Cluster może działać tylko z tabelami InnoDB, dlatego, żeby użyć klustra należy upewnić się, że wszystkie tabele spełniają ten wymóg.


\subsection{Podsumowanie}
W tym rozdziale przedstawiono kilka architektur możliwych do zrealizowania w MySQL. Przedstawione rozwiązania pokazują, że rozwiązanie wielu zagadnień związanych z optymalizacją może być rozwiązane na poziomie tworzenia architektury bazy danych. Poprzez zastosowanie odpowiedniej architektury możliwe jest zniwelowanie problemów, które występują przy wykorzystaniu pojedyńczego serwera bazy danych. 